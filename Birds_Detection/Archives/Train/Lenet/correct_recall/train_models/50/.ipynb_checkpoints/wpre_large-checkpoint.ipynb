{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Un problème de vp ???\n",
    "#image_2019-06-15_10-47-26_1112_64_2.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D'abord vérifier si on a du Lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import linear_model, preprocessing \n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model (width, height, depth, classes):\n",
    "\t# initialize the input shape and channels dimension to be\n",
    "\t# \"channels last\" ordering\n",
    "\tinputShape = (height, width, depth)\n",
    "\tchanDim = -1\n",
    "\t# build the model using Keras' Sequential API\n",
    "\tmodel = Sequential([\n",
    "\t\t# CONV => RELU => BN => POOL layer set\n",
    "\t\tConv2D(16, (3, 3), padding=\"same\", input_shape=inputShape),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
    "\t\t# (CONV => RELU => BN) * 2 => POOL layer set\n",
    "\t\tConv2D(32, (3, 3), padding=\"same\"),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tConv2D(32, (3, 3), padding=\"same\"),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
    "\t\t# (CONV => RELU => BN) * 3 => POOL layer set\n",
    "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
    "\t\t# first (and only) set of FC => RELU layers\n",
    "\t\tFlatten(),\n",
    "\t\tDense(256),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(),\n",
    "\t\tDropout(0.5),\n",
    "\t\t# softmax classifier\n",
    "\t\tDense(classes),\n",
    "\t\tActivation(\"softmax\")\n",
    "\t])\n",
    "\t# return the built model to the calling function\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paramètres\n",
    "\n",
    "#generateur_path='/mnt/VegaSlowDataDisk/c3po/Chaine_de_traitement/Train_imagettes_annotées/type_oiseau/Materiel/generateur.csv'\n",
    "test_size=0.2\n",
    "\n",
    "\n",
    "epochs=200\n",
    "batch_size = 600\n",
    "zoom_range = 1.25\n",
    "horizontal_flip = True\n",
    "Minimum_Number_Class=100\n",
    "dropout_rate=0.3\n",
    "#steps_per_epoch=len(data_train)//batch_size\n",
    "steps_per_epoch=1\n",
    "#validation_steps=len(data_test)//batch_size\n",
    "validation_steps=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: 13\n",
      "img_paths: 6220\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_img_paths=\"/home/marcpozzo/Desktop/c3po/Images_aquises/\"\n",
    "generateur_path='/mnt/VegaSlowDataDisk/c3po/Images_aquises/generateur_bigger.csv'\n",
    "\n",
    "df=pd.read_csv(generateur_path)\n",
    "df.drop('labels',inplace=True,axis=1)\n",
    "\n",
    "df[\"class\"].unique()\n",
    "for c in df:\n",
    "    print(''+c+':',len(df[c].unique()))\n",
    "df[\"class\"].unique()\n",
    "\n",
    "\n",
    "All_Unique=df[\"class\"].unique()\n",
    "Utilisable=[]\n",
    "for i in df[\"class\"].unique():\n",
    "    if df[\"class\"][df[\"class\"]==i].count()>Minimum_Number_Class:\n",
    "        Utilisable.append(i)\n",
    "Utilisable\n",
    "Non_Utilisable=set(All_Unique)-set(Utilisable)\n",
    "Non_Utilisable\n",
    "for i in Non_Utilisable:\n",
    "    df=df[df[\"class\"]!=i]\n",
    "df=df[df[\"class\"]!=\"oiseau\"]  \n",
    "df[\"class\"].unique()\n",
    "\n",
    "\n",
    "for i in range(len(df[\"class\"])):\n",
    "    image_name=df[\"img_paths\"].iloc[i]\n",
    "    df[\"img_paths\"].iloc[i]=os.path.join(base_img_paths,image_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcpozzo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_path=\"/mnt/VegaSlowDataDisk/c3po_interface/bin/fp_images/\"\n",
    "\n",
    "\n",
    "\n",
    "fp_new=pd.read_csv(\"/mnt/VegaSlowDataDisk/c3po_interface/bin/fp_threshold/table_fp_new.csv\")\n",
    "\n",
    "liste_img_paths=[]\n",
    "for i in range(len(fp_new)):\n",
    "    liste_img_paths.append(n_path+fp_new[\"imagetteName\"].iloc[i])\n",
    "\n",
    "fp_new[\"img_paths\"]=liste_img_paths\n",
    "#imagetteNamefp_new.head()\n",
    "\n",
    "fp_new[\"class\"]=\"autre\"\n",
    "fp_new.columns\n",
    "\n",
    "to_drop=['path', 'filename', 'imagetteName', 'max_cat', 'cat', 'xmin', 'xmax','ymin', 'ymax', 'former_index']\n",
    "\n",
    "fp_new.drop(to_drop,axis=1,inplace=True)\n",
    "\n",
    "df=pd.concat([df,fp_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40827 validated image filenames belonging to 6 classes.\n",
      "Found 10207 validated image filenames belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "data_train,data_test= train_test_split(df,stratify=df[\"class\"], test_size=test_size,random_state=42)\n",
    "train_data_generator = ImageDataGenerator(\n",
    "        preprocessing_function = preprocess_input\n",
    "        # data augmentation\n",
    "        #rotation_range = 10,\n",
    "        #zoom_range = zoom_range,\n",
    "        #horizontal_flip = horizontal_flip\n",
    "        )\n",
    "\n",
    "test_data_generator = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "    #preprocessing_function = preprocess_input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_generator = train_data_generator.flow_from_dataframe(dataframe=data_train,\n",
    "                                                          directory=\"\",\n",
    "                                                           x_col = \"img_paths\",\n",
    "                                                           class_mode =\"sparse\",\n",
    "                                                          target_size = (28 , 28), \n",
    "                                                          batch_size = len(data_train) )\n",
    "\n",
    "\n",
    "test_generator = test_data_generator.flow_from_dataframe(dataframe=data_test,\n",
    "                                                          directory=\"\",\n",
    "                                                           x_col = \"img_paths\",\n",
    "                                                           class_mode =\"sparse\",\n",
    "                                                          target_size = (28 , 28), \n",
    "                                                          batch_size = len(data_test))\n",
    "\n",
    "gen=train_generator[0]\n",
    "\n",
    "x_train=gen[0]\n",
    "y_train=gen[1]\n",
    "\n",
    "\n",
    "\n",
    "gen_test=test_generator[0]\n",
    "\n",
    "x_test=gen_test[0]\n",
    "y_test=gen_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "16608/40827 [===========>..................] - ETA: 22s - loss: 0.6164 - acc: 0.8322"
     ]
    }
   ],
   "source": [
    "val_acc=[]\n",
    "acc=[]\n",
    "val_loss=[]\n",
    "loss=[]\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "model = build_model(28, 28, 3, 6)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\",metrics=[\"acc\"])\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
