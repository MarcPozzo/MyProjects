{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Un problème de vp ???\n",
    "#image_2019-06-15_10-47-26_1112_64_2.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D'abord vérifier si on a du Lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import linear_model, preprocessing \n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Essayons de faire lenet avec tf voir ci-dessous . Il y a aussi une commande pour voir le résumer peut être .resume\n",
    "def build_model(width, height, depth, classes,drop_out_rate):\n",
    "\t# initialize the input shape and channels dimension to be\n",
    "\t# \"channels last\" ordering\n",
    "\tinputShape = (height, width, depth)\n",
    "\tchanDim = -1\n",
    "\t# build the model using Keras' Sequential API\n",
    "\tmodel = Sequential([\n",
    "\t\t# CONV => RELU => BN => POOL layer set\n",
    "\t\tConv2D(30, (5, 5), padding=\"same\", input_shape=inputShape),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "\t\t# (CONV => RELU => BN) * 2 => POOL layer set\n",
    "\t\tConv2D(16, (3, 3), padding=\"same\"),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        \n",
    "\t\t# first (and only) set of FC => RELU layers\n",
    "\t\tFlatten(),\n",
    "\t\tDense(128),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(),\n",
    "\t\tDropout(drop_out_rate),\n",
    "        \n",
    "\t\t# softmax classifier\n",
    "\t\tDense(classes),\n",
    "\t\tActivation(\"softmax\")\n",
    "\t])\n",
    "\t# return the built model to the calling function\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paramètres\n",
    "\n",
    "#generateur_path='/mnt/VegaSlowDataDisk/c3po/Chaine_de_traitement/Train_imagettes_annotées/type_oiseau/Materiel/generateur.csv'\n",
    "test_size=0.2\n",
    "\n",
    "\n",
    "epochs=200\n",
    "batch_size = 600\n",
    "zoom_range = 1.25\n",
    "horizontal_flip = True\n",
    "Minimum_Number_Class=100\n",
    "dropout_rate=0.3\n",
    "#steps_per_epoch=len(data_train)//batch_size\n",
    "steps_per_epoch=1\n",
    "#validation_steps=len(data_test)//batch_size\n",
    "validation_steps=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: 13\n",
      "img_paths: 6220\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_img_paths=\"/home/marcpozzo/Desktop/c3po/Images_aquises/\"\n",
    "generateur_path='/mnt/VegaSlowDataDisk/c3po/Images_aquises/generateur_bigger.csv'\n",
    "\n",
    "df=pd.read_csv(generateur_path)\n",
    "df.drop('labels',inplace=True,axis=1)\n",
    "\n",
    "df[\"class\"].unique()\n",
    "for c in df:\n",
    "    print(''+c+':',len(df[c].unique()))\n",
    "df[\"class\"].unique()\n",
    "\n",
    "\n",
    "All_Unique=df[\"class\"].unique()\n",
    "Utilisable=[]\n",
    "for i in df[\"class\"].unique():\n",
    "    if df[\"class\"][df[\"class\"]==i].count()>Minimum_Number_Class:\n",
    "        Utilisable.append(i)\n",
    "Utilisable\n",
    "Non_Utilisable=set(All_Unique)-set(Utilisable)\n",
    "Non_Utilisable\n",
    "for i in Non_Utilisable:\n",
    "    df=df[df[\"class\"]!=i]\n",
    "df=df[df[\"class\"]!=\"oiseau\"]  \n",
    "df[\"class\"].unique()\n",
    "\n",
    "\n",
    "for i in range(len(df[\"class\"])):\n",
    "    image_name=df[\"img_paths\"].iloc[i]\n",
    "    df[\"img_paths\"].iloc[i]=os.path.join(base_img_paths,image_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcpozzo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_path=\"/mnt/VegaSlowDataDisk/c3po_interface/bin/fp_images/\"\n",
    "\n",
    "\n",
    "\n",
    "fp_new=pd.read_csv(\"/mnt/VegaSlowDataDisk/c3po_interface/bin/fp_threshold/table_fp_new.csv\")\n",
    "\n",
    "liste_img_paths=[]\n",
    "for i in range(len(fp_new)):\n",
    "    liste_img_paths.append(n_path+fp_new[\"imagetteName\"].iloc[i])\n",
    "\n",
    "fp_new[\"img_paths\"]=liste_img_paths\n",
    "#imagetteNamefp_new.head()\n",
    "\n",
    "fp_new[\"class\"]=\"autre\"\n",
    "fp_new.columns\n",
    "\n",
    "to_drop=['path', 'filename', 'imagetteName', 'max_cat', 'cat', 'xmin', 'xmax','ymin', 'ymax', 'former_index']\n",
    "\n",
    "fp_new.drop(to_drop,axis=1,inplace=True)\n",
    "\n",
    "df=pd.concat([df,fp_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40827 validated image filenames belonging to 6 classes.\n",
      "Found 10207 validated image filenames belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "data_train,data_test= train_test_split(df,stratify=df[\"class\"], test_size=test_size,random_state=42)\n",
    "train_data_generator = ImageDataGenerator()\n",
    "\n",
    "test_data_generator = ImageDataGenerator()\n",
    "    #preprocessing_function = preprocess_input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_generator = train_data_generator.flow_from_dataframe(dataframe=data_train,\n",
    "                                                          directory=\"\",\n",
    "                                                           x_col = \"img_paths\",\n",
    "                                                           class_mode =\"sparse\",\n",
    "                                                          target_size = (28 , 28), \n",
    "                                                          batch_size = len(data_train) )\n",
    "\n",
    "\n",
    "test_generator = test_data_generator.flow_from_dataframe(dataframe=data_test,\n",
    "                                                          directory=\"\",\n",
    "                                                           x_col = \"img_paths\",\n",
    "                                                           class_mode =\"sparse\",\n",
    "                                                          target_size = (28 , 28), \n",
    "                                                          batch_size = len(data_test))\n",
    "\n",
    "gen=train_generator[0]\n",
    "\n",
    "x_train=gen[0]\n",
    "y_train=gen[1]\n",
    "\n",
    "\n",
    "\n",
    "gen_test=test_generator[0]\n",
    "\n",
    "x_test=gen_test[0]\n",
    "y_test=gen_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "40827/40827 [==============================] - 26s 629us/sample - loss: 0.3534 - acc: 0.9083 - val_loss: 0.2512 - val_acc: 0.9491\n",
      "Epoch 2/10\n",
      "40827/40827 [==============================] - 25s 612us/sample - loss: 0.1577 - acc: 0.9581 - val_loss: 0.1791 - val_acc: 0.9575\n",
      "Epoch 3/10\n",
      "40827/40827 [==============================] - 25s 601us/sample - loss: 0.1278 - acc: 0.9634 - val_loss: 0.1388 - val_acc: 0.9638\n",
      "Epoch 4/10\n",
      "40827/40827 [==============================] - 25s 618us/sample - loss: 0.1082 - acc: 0.9680 - val_loss: 0.2361 - val_acc: 0.9503\n",
      "Epoch 5/10\n",
      "40827/40827 [==============================] - 25s 612us/sample - loss: 0.0991 - acc: 0.9703 - val_loss: 0.1538 - val_acc: 0.9608\n",
      "Epoch 6/10\n",
      "40827/40827 [==============================] - 29s 707us/sample - loss: 0.0920 - acc: 0.9724 - val_loss: 0.1036 - val_acc: 0.9729\n",
      "Epoch 7/10\n",
      "40827/40827 [==============================] - 29s 705us/sample - loss: 0.0834 - acc: 0.9743 - val_loss: 0.1328 - val_acc: 0.9680\n",
      "Epoch 8/10\n",
      "40827/40827 [==============================] - 29s 702us/sample - loss: 0.0770 - acc: 0.9761 - val_loss: 0.1550 - val_acc: 0.9641\n",
      "Epoch 9/10\n",
      "40827/40827 [==============================] - 28s 695us/sample - loss: 0.0725 - acc: 0.9770 - val_loss: 0.1923 - val_acc: 0.9625\n",
      "Epoch 10/10\n",
      "40827/40827 [==============================] - 28s 692us/sample - loss: 0.0715 - acc: 0.9775 - val_loss: 0.1560 - val_acc: 0.9624\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99773   0.96480   0.98099     10028\n",
      "           1    0.25926   0.36842   0.30435        19\n",
      "           2    0.40000   0.92473   0.55844        93\n",
      "           3    0.17910   0.85714   0.29630        14\n",
      "           4    0.11429   0.66667   0.19512         6\n",
      "           5    0.23494   0.82979   0.36620        47\n",
      "\n",
      "    accuracy                        0.96238     10207\n",
      "   macro avg    0.36422   0.76859   0.45023     10207\n",
      "weighted avg    0.98576   0.96238   0.97165     10207\n",
      "\n",
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "40827/40827 [==============================] - 30s 727us/sample - loss: 0.0595 - acc: 0.9811 - val_loss: 0.0944 - val_acc: 0.9755\n",
      "Epoch 2/10\n",
      "40827/40827 [==============================] - 29s 722us/sample - loss: 0.0566 - acc: 0.9816 - val_loss: 0.0748 - val_acc: 0.9812\n",
      "Epoch 3/10\n",
      "40827/40827 [==============================] - 35s 851us/sample - loss: 0.0580 - acc: 0.9813 - val_loss: 0.1508 - val_acc: 0.9677\n",
      "Epoch 4/10\n",
      "40827/40827 [==============================] - 34s 828us/sample - loss: 0.0508 - acc: 0.9833 - val_loss: 0.1044 - val_acc: 0.9737\n",
      "Epoch 5/10\n",
      "40827/40827 [==============================] - 33s 820us/sample - loss: 0.0506 - acc: 0.9829 - val_loss: 0.1427 - val_acc: 0.9666\n",
      "Epoch 6/10\n",
      "40827/40827 [==============================] - 34s 844us/sample - loss: 0.0486 - acc: 0.9847 - val_loss: 0.0668 - val_acc: 0.9843\n",
      "Epoch 7/10\n",
      "40827/40827 [==============================] - 33s 799us/sample - loss: 0.0435 - acc: 0.9859 - val_loss: 0.0634 - val_acc: 0.9835\n",
      "Epoch 8/10\n",
      "40827/40827 [==============================] - 32s 794us/sample - loss: 0.0430 - acc: 0.9855 - val_loss: 0.0667 - val_acc: 0.9825\n",
      "Epoch 9/10\n",
      "40827/40827 [==============================] - 32s 776us/sample - loss: 0.0422 - acc: 0.9863 - val_loss: 0.0778 - val_acc: 0.9818\n",
      "Epoch 10/10\n",
      "40827/40827 [==============================] - 33s 798us/sample - loss: 0.0406 - acc: 0.9862 - val_loss: 0.0651 - val_acc: 0.9851\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99526   0.99025   0.99275      9746\n",
      "           1    0.74074   0.86957   0.80000        23\n",
      "           2    0.85581   0.91089   0.88249       202\n",
      "           3    0.58209   0.92857   0.71560        42\n",
      "           4    0.51429   0.85714   0.64286        21\n",
      "           5    0.86145   0.82659   0.84366       173\n",
      "\n",
      "    accuracy                        0.98511     10207\n",
      "   macro avg    0.75827   0.89717   0.81289     10207\n",
      "weighted avg    0.98697   0.98511   0.98574     10207\n",
      "\n",
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "40827/40827 [==============================] - 33s 799us/sample - loss: 0.0365 - acc: 0.9875 - val_loss: 0.1430 - val_acc: 0.9813\n",
      "Epoch 2/10\n",
      "40827/40827 [==============================] - 32s 789us/sample - loss: 0.0355 - acc: 0.9880 - val_loss: 0.0640 - val_acc: 0.9840\n",
      "Epoch 3/10\n",
      "40827/40827 [==============================] - 33s 804us/sample - loss: 0.0382 - acc: 0.9874 - val_loss: 0.0752 - val_acc: 0.9821\n",
      "Epoch 4/10\n",
      "40827/40827 [==============================] - 32s 790us/sample - loss: 0.0331 - acc: 0.9883 - val_loss: 0.0630 - val_acc: 0.9841\n",
      "Epoch 5/10\n",
      "40827/40827 [==============================] - 33s 798us/sample - loss: 0.0321 - acc: 0.9888 - val_loss: 0.0664 - val_acc: 0.9840\n",
      "Epoch 6/10\n",
      "40827/40827 [==============================] - 32s 790us/sample - loss: 0.0310 - acc: 0.9895 - val_loss: 0.0650 - val_acc: 0.9824\n",
      "Epoch 7/10\n",
      "40827/40827 [==============================] - 32s 789us/sample - loss: 0.0287 - acc: 0.9900 - val_loss: 0.1345 - val_acc: 0.9727\n",
      "Epoch 8/10\n",
      "40827/40827 [==============================] - 32s 784us/sample - loss: 0.0290 - acc: 0.9897 - val_loss: 0.0818 - val_acc: 0.9768\n",
      "Epoch 9/10\n",
      "40827/40827 [==============================] - 33s 798us/sample - loss: 0.0293 - acc: 0.9903 - val_loss: 0.2131 - val_acc: 0.9720\n",
      "Epoch 10/10\n",
      "40827/40827 [==============================] - 32s 786us/sample - loss: 0.0297 - acc: 0.9893 - val_loss: 0.0717 - val_acc: 0.9845\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99577   0.99026   0.99301      9751\n",
      "           1    0.85185   0.82143   0.83636        28\n",
      "           2    0.77674   0.91257   0.83920       183\n",
      "           3    0.53731   0.90000   0.67290        40\n",
      "           4    0.62857   0.68750   0.65672        32\n",
      "           5    0.87349   0.83815   0.85546       173\n",
      "\n",
      "    accuracy                        0.98452     10207\n",
      "   macro avg    0.77729   0.85832   0.80894     10207\n",
      "weighted avg    0.98643   0.98452   0.98518     10207\n",
      "\n",
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "40827/40827 [==============================] - 32s 794us/sample - loss: 0.0237 - acc: 0.9916 - val_loss: 0.0777 - val_acc: 0.9826\n",
      "Epoch 2/10\n",
      "40827/40827 [==============================] - 32s 781us/sample - loss: 0.0237 - acc: 0.9919 - val_loss: 0.0938 - val_acc: 0.9826\n",
      "Epoch 3/10\n",
      "40827/40827 [==============================] - 32s 780us/sample - loss: 0.0291 - acc: 0.9903 - val_loss: 0.0759 - val_acc: 0.9839\n",
      "Epoch 4/10\n",
      "40827/40827 [==============================] - 33s 800us/sample - loss: 0.0232 - acc: 0.9920 - val_loss: 0.0883 - val_acc: 0.9804\n",
      "Epoch 5/10\n",
      "40827/40827 [==============================] - 32s 795us/sample - loss: 0.0228 - acc: 0.9923 - val_loss: 0.1571 - val_acc: 0.9744\n",
      "Epoch 6/10\n",
      "40827/40827 [==============================] - 32s 773us/sample - loss: 0.0249 - acc: 0.9917 - val_loss: 0.0676 - val_acc: 0.9853\n",
      "Epoch 7/10\n",
      "40827/40827 [==============================] - 32s 790us/sample - loss: 0.0211 - acc: 0.9928 - val_loss: 0.0717 - val_acc: 0.9831\n",
      "Epoch 8/10\n",
      "40827/40827 [==============================] - 32s 791us/sample - loss: 0.0237 - acc: 0.9917 - val_loss: 0.0695 - val_acc: 0.9843\n",
      "Epoch 9/10\n",
      "40827/40827 [==============================] - 34s 831us/sample - loss: 0.0200 - acc: 0.9931 - val_loss: 0.0703 - val_acc: 0.9851\n",
      "Epoch 10/10\n",
      "40827/40827 [==============================] - 33s 817us/sample - loss: 0.0194 - acc: 0.9930 - val_loss: 0.0903 - val_acc: 0.9832\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99598   0.98783   0.99189      9777\n",
      "           1    0.77778   0.87500   0.82353        24\n",
      "           2    0.80000   0.86869   0.83293       198\n",
      "           3    0.53731   0.92308   0.67925        39\n",
      "           4    0.60000   0.87500   0.71186        24\n",
      "           5    0.77108   0.88276   0.82315       145\n",
      "\n",
      "    accuracy                        0.98325     10207\n",
      "   macro avg    0.74703   0.90206   0.81043     10207\n",
      "weighted avg    0.98578   0.98325   0.98416     10207\n",
      "\n",
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "40827/40827 [==============================] - 33s 820us/sample - loss: 0.0192 - acc: 0.9929 - val_loss: 0.0848 - val_acc: 0.9801\n",
      "Epoch 2/10\n",
      "40827/40827 [==============================] - 33s 798us/sample - loss: 0.0162 - acc: 0.9947 - val_loss: 0.0876 - val_acc: 0.9829\n",
      "Epoch 3/10\n",
      "40827/40827 [==============================] - 32s 789us/sample - loss: 0.0191 - acc: 0.9936 - val_loss: 0.0985 - val_acc: 0.9805\n",
      "Epoch 4/10\n",
      "40827/40827 [==============================] - 32s 792us/sample - loss: 0.0181 - acc: 0.9936 - val_loss: 0.0887 - val_acc: 0.9840\n",
      "Epoch 5/10\n",
      "40827/40827 [==============================] - 32s 791us/sample - loss: 0.0174 - acc: 0.9941 - val_loss: 0.0785 - val_acc: 0.9842\n",
      "Epoch 6/10\n",
      "40827/40827 [==============================] - 32s 792us/sample - loss: 0.0191 - acc: 0.9937 - val_loss: 0.0859 - val_acc: 0.9855\n",
      "Epoch 7/10\n",
      "40827/40827 [==============================] - 32s 796us/sample - loss: 0.0157 - acc: 0.9945 - val_loss: 0.1218 - val_acc: 0.9788\n",
      "Epoch 8/10\n",
      "40827/40827 [==============================] - 32s 790us/sample - loss: 0.0163 - acc: 0.9945 - val_loss: 0.0821 - val_acc: 0.9824\n",
      "Epoch 9/10\n",
      "40827/40827 [==============================] - 33s 801us/sample - loss: 0.0165 - acc: 0.9943 - val_loss: 0.1021 - val_acc: 0.9833\n",
      "Epoch 10/10\n",
      "40827/40827 [==============================] - 32s 794us/sample - loss: 0.0167 - acc: 0.9937 - val_loss: 0.0887 - val_acc: 0.9857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99557   0.99097   0.99326      9742\n",
      "           1    0.88889   0.75000   0.81356        32\n",
      "           2    0.85116   0.88406   0.86730       207\n",
      "           3    0.58209   0.92857   0.71560        42\n",
      "           4    0.62857   0.84615   0.72131        26\n",
      "           5    0.83735   0.87975   0.85802       158\n",
      "\n",
      "    accuracy                        0.98570     10207\n",
      "   macro avg    0.79727   0.87992   0.82818     10207\n",
      "weighted avg    0.98722   0.98570   0.98621     10207\n",
      "\n",
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "40827/40827 [==============================] - 32s 794us/sample - loss: 0.0148 - acc: 0.9947 - val_loss: 0.0907 - val_acc: 0.9839\n",
      "Epoch 2/10\n",
      "40827/40827 [==============================] - 33s 799us/sample - loss: 0.0135 - acc: 0.9952 - val_loss: 0.0929 - val_acc: 0.9844\n",
      "Epoch 3/10\n",
      "40827/40827 [==============================] - 32s 790us/sample - loss: 0.0145 - acc: 0.9948 - val_loss: 0.0847 - val_acc: 0.9840\n",
      "Epoch 4/10\n",
      "40827/40827 [==============================] - 32s 785us/sample - loss: 0.0154 - acc: 0.9949 - val_loss: 0.0895 - val_acc: 0.9853\n",
      "Epoch 5/10\n",
      "40827/40827 [==============================] - 32s 785us/sample - loss: 0.0144 - acc: 0.9951 - val_loss: 0.1109 - val_acc: 0.9860\n",
      "Epoch 6/10\n",
      "40827/40827 [==============================] - 33s 806us/sample - loss: 0.0145 - acc: 0.9949 - val_loss: 0.0936 - val_acc: 0.9835\n",
      "Epoch 7/10\n",
      "40827/40827 [==============================] - 34s 841us/sample - loss: 0.0111 - acc: 0.9961 - val_loss: 0.1519 - val_acc: 0.9774\n",
      "Epoch 8/10\n",
      "40827/40827 [==============================] - 34s 829us/sample - loss: 0.0134 - acc: 0.9951 - val_loss: 0.1240 - val_acc: 0.9808\n",
      "Epoch 9/10\n",
      "40827/40827 [==============================] - 33s 808us/sample - loss: 0.0142 - acc: 0.9951 - val_loss: 0.0846 - val_acc: 0.9869\n",
      "Epoch 10/10\n",
      "40827/40827 [==============================] - 33s 817us/sample - loss: 0.0116 - acc: 0.9959 - val_loss: 0.0810 - val_acc: 0.9858\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99557   0.99158   0.99357      9736\n",
      "           1    0.77778   0.67742   0.72414        31\n",
      "           2    0.83721   0.89109   0.86331       202\n",
      "           3    0.64179   0.87755   0.74138        49\n",
      "           4    0.62857   0.81481   0.70968        27\n",
      "           5    0.85542   0.87654   0.86585       162\n",
      "\n",
      "    accuracy                        0.98579     10207\n",
      "   macro avg    0.78939   0.85483   0.81632     10207\n",
      "weighted avg    0.98688   0.98579   0.98618     10207\n",
      "\n",
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "40827/40827 [==============================] - 35s 849us/sample - loss: 0.0118 - acc: 0.9959 - val_loss: 0.0932 - val_acc: 0.9837\n",
      "Epoch 2/10\n",
      "40827/40827 [==============================] - 34s 830us/sample - loss: 0.0110 - acc: 0.9962 - val_loss: 0.1018 - val_acc: 0.9833\n",
      "Epoch 3/10\n",
      "40827/40827 [==============================] - 34s 830us/sample - loss: 0.0123 - acc: 0.9957 - val_loss: 0.1378 - val_acc: 0.9794\n",
      "Epoch 4/10\n",
      "40827/40827 [==============================] - 34s 829us/sample - loss: 0.0116 - acc: 0.9959 - val_loss: 0.1156 - val_acc: 0.9828\n",
      "Epoch 5/10\n",
      "40827/40827 [==============================] - 34s 821us/sample - loss: 0.0114 - acc: 0.9958 - val_loss: 0.0847 - val_acc: 0.9855\n",
      "Epoch 6/10\n",
      "40827/40827 [==============================] - 35s 853us/sample - loss: 0.0113 - acc: 0.9962 - val_loss: 0.0966 - val_acc: 0.9837\n",
      "Epoch 7/10\n",
      "40827/40827 [==============================] - 33s 808us/sample - loss: 0.0106 - acc: 0.9963 - val_loss: 0.0954 - val_acc: 0.9844\n",
      "Epoch 8/10\n",
      "40827/40827 [==============================] - 34s 827us/sample - loss: 0.0100 - acc: 0.9967 - val_loss: 0.1025 - val_acc: 0.9820\n",
      "Epoch 9/10\n",
      "40827/40827 [==============================] - 34s 833us/sample - loss: 0.0121 - acc: 0.9959 - val_loss: 0.0952 - val_acc: 0.9840\n",
      "Epoch 10/10\n",
      "40827/40827 [==============================] - 34s 839us/sample - loss: 0.0107 - acc: 0.9961 - val_loss: 0.0847 - val_acc: 0.9844\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99474   0.99045   0.99259      9739\n",
      "           1    0.70370   0.79167   0.74510        24\n",
      "           2    0.81860   0.89796   0.85645       196\n",
      "           3    0.62687   0.91304   0.74336        46\n",
      "           4    0.62857   0.88000   0.73333        25\n",
      "           5    0.86145   0.80791   0.83382       177\n",
      "\n",
      "    accuracy                        0.98442     10207\n",
      "   macro avg    0.77232   0.88017   0.81744     10207\n",
      "weighted avg    0.98581   0.98442   0.98488     10207\n",
      "\n",
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "40827/40827 [==============================] - 35s 848us/sample - loss: 0.0094 - acc: 0.9963 - val_loss: 0.1206 - val_acc: 0.9823\n",
      "Epoch 2/10\n",
      "40827/40827 [==============================] - 35s 848us/sample - loss: 0.0090 - acc: 0.9967 - val_loss: 0.1035 - val_acc: 0.9844\n",
      "Epoch 3/10\n",
      "40827/40827 [==============================] - 34s 834us/sample - loss: 0.0099 - acc: 0.9966 - val_loss: 0.1208 - val_acc: 0.9824\n",
      "Epoch 4/10\n",
      "40827/40827 [==============================] - 34s 841us/sample - loss: 0.0093 - acc: 0.9966 - val_loss: 0.0944 - val_acc: 0.9853\n",
      "Epoch 5/10\n",
      "40827/40827 [==============================] - 35s 848us/sample - loss: 0.0093 - acc: 0.9968 - val_loss: 0.1022 - val_acc: 0.9848\n",
      "Epoch 6/10\n",
      "40827/40827 [==============================] - 34s 835us/sample - loss: 0.0098 - acc: 0.9967 - val_loss: 0.0940 - val_acc: 0.9864\n",
      "Epoch 7/10\n",
      "40827/40827 [==============================] - 34s 821us/sample - loss: 0.0078 - acc: 0.9970 - val_loss: 0.0910 - val_acc: 0.9864\n",
      "Epoch 8/10\n",
      "40827/40827 [==============================] - 34s 832us/sample - loss: 0.0093 - acc: 0.9968 - val_loss: 0.1625 - val_acc: 0.9781\n",
      "Epoch 9/10\n",
      "40827/40827 [==============================] - 33s 816us/sample - loss: 0.0091 - acc: 0.9968 - val_loss: 0.1059 - val_acc: 0.9846\n",
      "Epoch 10/10\n",
      "40800/40827 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9970"
     ]
    }
   ],
   "source": [
    "val_acc=[]\n",
    "acc=[]\n",
    "val_loss=[]\n",
    "loss=[]\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "model = build_model(28, 28, 3, 6,0.3)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\",metrics=[\"acc\"])\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "val_acc_liste\n",
    "acc_liste\n",
    "\n",
    "\n",
    "val_acc_liste = list(itertools.chain(*val_acc))\n",
    "acc_liste = list(itertools.chain(*acc))\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "plt.plot(acc_liste,label=\"train\");\n",
    "plt.plot(val_acc_liste,label=\"test\");\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Accuracy en fonction du nombre d'epoch\");\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_loss_liste = list(itertools.chain(*val_loss))\n",
    "loss_liste = list(itertools.chain(*loss))\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "plt.plot(loss_liste,label=\"train\");\n",
    "plt.plot(val_loss_liste,label=\"test\");\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Accuracy en fonction du nombre d'epoch\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#99158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
