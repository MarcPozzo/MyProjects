{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Un problème de vp ???\n",
    "#image_2019-06-15_10-47-26_1112_64_2.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D'abord vérifier si on a du Lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import linear_model, preprocessing \n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Essayons de faire lenet avec tf voir ci-dessous . Il y a aussi une commande pour voir le résumer peut être .resume\n",
    "def build_model(width, height, depth, classes,drop_out_rate):\n",
    "\t# initialize the input shape and channels dimension to be\n",
    "\t# \"channels last\" ordering\n",
    "\tinputShape = (height, width, depth)\n",
    "\tchanDim = -1\n",
    "\t# build the model using Keras' Sequential API\n",
    "\tmodel = Sequential([\n",
    "\t\t# CONV => RELU => BN => POOL layer set\n",
    "\t\tConv2D(30, (5, 5), padding=\"same\", input_shape=inputShape),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "\t\t# (CONV => RELU => BN) * 2 => POOL layer set\n",
    "\t\tConv2D(16, (3, 3), padding=\"same\"),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(axis=chanDim),\n",
    "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        \n",
    "\t\t# first (and only) set of FC => RELU layers\n",
    "\t\tFlatten(),\n",
    "\t\tDense(128),\n",
    "\t\tActivation(\"relu\"),\n",
    "\t\tBatchNormalization(),\n",
    "\t\tDropout(drop_out_rate),\n",
    "        \n",
    "\t\t# softmax classifier\n",
    "\t\tDense(classes),\n",
    "\t\tActivation(\"softmax\")\n",
    "\t])\n",
    "\t# return the built model to the calling function\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paramètres\n",
    "\n",
    "#generateur_path='/mnt/VegaSlowDataDisk/c3po/Chaine_de_traitement/Train_imagettes_annotées/type_oiseau/Materiel/generateur.csv'\n",
    "test_size=0.2\n",
    "\n",
    "\n",
    "epochs=200\n",
    "batch_size = 600\n",
    "zoom_range = 1.25\n",
    "horizontal_flip = True\n",
    "Minimum_Number_Class=100\n",
    "dropout_rate=0.3\n",
    "#steps_per_epoch=len(data_train)//batch_size\n",
    "steps_per_epoch=1\n",
    "#validation_steps=len(data_test)//batch_size\n",
    "validation_steps=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: 13\n",
      "img_paths: 6220\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_img_paths=\"/home/marcpozzo/Desktop/c3po/Images_aquises/\"\n",
    "generateur_path='/mnt/VegaSlowDataDisk/c3po/Images_aquises/generateur_bigger.csv'\n",
    "\n",
    "df=pd.read_csv(generateur_path)\n",
    "df.drop('labels',inplace=True,axis=1)\n",
    "\n",
    "df[\"class\"].unique()\n",
    "for c in df:\n",
    "    print(''+c+':',len(df[c].unique()))\n",
    "df[\"class\"].unique()\n",
    "\n",
    "\n",
    "All_Unique=df[\"class\"].unique()\n",
    "Utilisable=[]\n",
    "for i in df[\"class\"].unique():\n",
    "    if df[\"class\"][df[\"class\"]==i].count()>Minimum_Number_Class:\n",
    "        Utilisable.append(i)\n",
    "Utilisable\n",
    "Non_Utilisable=set(All_Unique)-set(Utilisable)\n",
    "Non_Utilisable\n",
    "for i in Non_Utilisable:\n",
    "    df=df[df[\"class\"]!=i]\n",
    "df=df[df[\"class\"]!=\"oiseau\"]  \n",
    "df[\"class\"].unique()\n",
    "\n",
    "\n",
    "for i in range(len(df[\"class\"])):\n",
    "    image_name=df[\"img_paths\"].iloc[i]\n",
    "    df[\"img_paths\"].iloc[i]=os.path.join(base_img_paths,image_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcpozzo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_path=\"/mnt/VegaSlowDataDisk/c3po_interface/bin/fp_images/\"\n",
    "\n",
    "\n",
    "\n",
    "fp_new=pd.read_csv(\"/mnt/VegaSlowDataDisk/c3po_interface/bin/fp_threshold/table_fp_new.csv\")\n",
    "\n",
    "liste_img_paths=[]\n",
    "for i in range(len(fp_new)):\n",
    "    liste_img_paths.append(n_path+fp_new[\"imagetteName\"].iloc[i])\n",
    "\n",
    "fp_new[\"img_paths\"]=liste_img_paths\n",
    "#imagetteNamefp_new.head()\n",
    "\n",
    "fp_new[\"class\"]=\"autre\"\n",
    "fp_new.columns\n",
    "\n",
    "to_drop=['path', 'filename', 'imagetteName', 'max_cat', 'cat', 'xmin', 'xmax','ymin', 'ymax', 'former_index']\n",
    "\n",
    "fp_new.drop(to_drop,axis=1,inplace=True)\n",
    "\n",
    "df=pd.concat([df,fp_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40827 validated image filenames belonging to 6 classes.\n",
      "Found 10207 validated image filenames belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "data_train,data_test= train_test_split(df,stratify=df[\"class\"], test_size=test_size,random_state=42)\n",
    "train_data_generator = ImageDataGenerator(\n",
    "        preprocessing_function = preprocess_input\n",
    "        # data augmentation\n",
    "        #rotation_range = 10,\n",
    "        #zoom_range = zoom_range,\n",
    "        #horizontal_flip = horizontal_flip\n",
    "        )\n",
    "\n",
    "test_data_generator = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "    #preprocessing_function = preprocess_input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_generator = train_data_generator.flow_from_dataframe(dataframe=data_train,\n",
    "                                                          directory=\"\",\n",
    "                                                           x_col = \"img_paths\",\n",
    "                                                           class_mode =\"sparse\",\n",
    "                                                          target_size = (28 , 28), \n",
    "                                                          batch_size = len(data_train) )\n",
    "\n",
    "\n",
    "test_generator = test_data_generator.flow_from_dataframe(dataframe=data_test,\n",
    "                                                          directory=\"\",\n",
    "                                                           x_col = \"img_paths\",\n",
    "                                                           class_mode =\"sparse\",\n",
    "                                                          target_size = (28 , 28), \n",
    "                                                          batch_size = len(data_test))\n",
    "\n",
    "gen=train_generator[0]\n",
    "\n",
    "x_train=gen[0]\n",
    "y_train=gen[1]\n",
    "\n",
    "\n",
    "\n",
    "gen_test=test_generator[0]\n",
    "\n",
    "x_test=gen_test[0]\n",
    "y_test=gen_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "40827/40827 [==============================] - 24s 592us/sample - loss: 0.3504 - acc: 0.9061 - val_loss: 0.1371 - val_acc: 0.9572\n",
      "Epoch 2/10\n",
      "40827/40827 [==============================] - 24s 582us/sample - loss: 0.1438 - acc: 0.9614 - val_loss: 0.1041 - val_acc: 0.9711\n",
      "Epoch 3/10\n",
      "40827/40827 [==============================] - 24s 580us/sample - loss: 0.1116 - acc: 0.9672 - val_loss: 0.1053 - val_acc: 0.9675\n",
      "Epoch 4/10\n",
      "40827/40827 [==============================] - 23s 575us/sample - loss: 0.0934 - acc: 0.9715 - val_loss: 0.0741 - val_acc: 0.9783\n",
      "Epoch 5/10\n",
      "40827/40827 [==============================] - 24s 584us/sample - loss: 0.0800 - acc: 0.9749 - val_loss: 0.0730 - val_acc: 0.9785\n",
      "Epoch 6/10\n",
      "40827/40827 [==============================] - 24s 580us/sample - loss: 0.0733 - acc: 0.9780 - val_loss: 0.0640 - val_acc: 0.9830\n",
      "Epoch 7/10\n",
      "40827/40827 [==============================] - 23s 570us/sample - loss: 0.0662 - acc: 0.9788 - val_loss: 0.0728 - val_acc: 0.9772\n",
      "Epoch 8/10\n",
      "40827/40827 [==============================] - 24s 579us/sample - loss: 0.0634 - acc: 0.9794 - val_loss: 0.0672 - val_acc: 0.9797\n",
      "Epoch 9/10\n",
      "40827/40827 [==============================] - 25s 601us/sample - loss: 0.0575 - acc: 0.9812 - val_loss: 0.0736 - val_acc: 0.9771\n",
      "Epoch 10/10\n",
      "40827/40827 [==============================] - 24s 594us/sample - loss: 0.0517 - acc: 0.9824 - val_loss: 0.0605 - val_acc: 0.9841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99423   0.99116   0.99269      9727\n",
      "           1    0.62963   0.94444   0.75556        18\n",
      "           2    0.88372   0.82251   0.85202       231\n",
      "           3    0.62687   0.91304   0.74336        46\n",
      "           4    0.60000   0.84000   0.70000        25\n",
      "           5    0.80723   0.83750   0.82209       160\n",
      "\n",
      "    accuracy                        0.98413     10207\n",
      "   macro avg    0.75695   0.89144   0.81095     10207\n",
      "weighted avg    0.98553   0.98413   0.98457     10207\n",
      "\n",
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "40827/40827 [==============================] - 24s 595us/sample - loss: 0.0481 - acc: 0.9846 - val_loss: 0.0592 - val_acc: 0.9848\n",
      "Epoch 2/10\n",
      "40827/40827 [==============================] - 24s 596us/sample - loss: 0.0433 - acc: 0.9860 - val_loss: 0.0618 - val_acc: 0.9831\n",
      "Epoch 3/10\n",
      "40827/40827 [==============================] - 26s 637us/sample - loss: 0.0425 - acc: 0.9861 - val_loss: 0.0574 - val_acc: 0.9836\n",
      "Epoch 4/10\n",
      "40827/40827 [==============================] - 27s 673us/sample - loss: 0.0386 - acc: 0.9866 - val_loss: 0.0555 - val_acc: 0.9835\n",
      "Epoch 5/10\n",
      "40827/40827 [==============================] - 28s 683us/sample - loss: 0.0359 - acc: 0.9882 - val_loss: 0.0682 - val_acc: 0.9820\n",
      "Epoch 6/10\n",
      "40827/40827 [==============================] - 27s 653us/sample - loss: 0.0351 - acc: 0.9878 - val_loss: 0.0627 - val_acc: 0.9832\n",
      "Epoch 7/10\n",
      "40827/40827 [==============================] - 27s 663us/sample - loss: 0.0310 - acc: 0.9892 - val_loss: 0.0570 - val_acc: 0.9850\n",
      "Epoch 8/10\n",
      "40827/40827 [==============================] - 27s 653us/sample - loss: 0.0314 - acc: 0.9886 - val_loss: 0.0563 - val_acc: 0.9855\n",
      "Epoch 9/10\n",
      "40827/40827 [==============================] - 28s 685us/sample - loss: 0.0293 - acc: 0.9890 - val_loss: 0.0633 - val_acc: 0.9843\n",
      "Epoch 10/10\n",
      "40827/40827 [==============================] - 28s 693us/sample - loss: 0.0280 - acc: 0.9900 - val_loss: 0.0657 - val_acc: 0.9835\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99773   0.98624   0.99195      9810\n",
      "           1    0.70370   0.95000   0.80851        20\n",
      "           2    0.73953   0.92442   0.82171       172\n",
      "           3    0.59701   0.93023   0.72727        43\n",
      "           4    0.57143   0.86957   0.68966        23\n",
      "           5    0.75904   0.90647   0.82623       139\n",
      "\n",
      "    accuracy                        0.98354     10207\n",
      "   macro avg    0.72807   0.92782   0.81089     10207\n",
      "weighted avg    0.98690   0.98354   0.98467     10207\n",
      "\n",
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "40827/40827 [==============================] - 35s 847us/sample - loss: 0.0271 - acc: 0.9908 - val_loss: 0.0622 - val_acc: 0.9840\n",
      "Epoch 2/10\n",
      "40827/40827 [==============================] - 34s 827us/sample - loss: 0.0237 - acc: 0.9920 - val_loss: 0.0709 - val_acc: 0.9844\n",
      "Epoch 3/10\n",
      "40827/40827 [==============================] - 33s 821us/sample - loss: 0.0240 - acc: 0.9917 - val_loss: 0.0732 - val_acc: 0.9831\n",
      "Epoch 4/10\n",
      "40827/40827 [==============================] - 34s 832us/sample - loss: 0.0221 - acc: 0.9923 - val_loss: 0.0679 - val_acc: 0.9854\n",
      "Epoch 5/10\n",
      "40827/40827 [==============================] - 33s 797us/sample - loss: 0.0230 - acc: 0.9920 - val_loss: 0.0708 - val_acc: 0.9858\n",
      "Epoch 6/10\n",
      "40827/40827 [==============================] - 33s 797us/sample - loss: 0.0223 - acc: 0.9925 - val_loss: 0.0632 - val_acc: 0.9866\n",
      "Epoch 7/10\n",
      "40827/40827 [==============================] - 32s 775us/sample - loss: 0.0206 - acc: 0.9931 - val_loss: 0.0663 - val_acc: 0.9862\n",
      "Epoch 8/10\n",
      "40827/40827 [==============================] - 32s 793us/sample - loss: 0.0205 - acc: 0.9929 - val_loss: 0.0640 - val_acc: 0.9869\n",
      "Epoch 9/10\n",
      "40827/40827 [==============================] - 32s 789us/sample - loss: 0.0218 - acc: 0.9918 - val_loss: 0.0637 - val_acc: 0.9863\n",
      "Epoch 10/10\n",
      "40827/40827 [==============================] - 32s 795us/sample - loss: 0.0173 - acc: 0.9940 - val_loss: 0.0740 - val_acc: 0.9861\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99536   0.99219   0.99377      9728\n",
      "           1    0.77778   0.91304   0.84000        23\n",
      "           2    0.88372   0.86364   0.87356       220\n",
      "           3    0.67164   0.81818   0.73770        55\n",
      "           4    0.54286   0.82609   0.65517        23\n",
      "           5    0.83133   0.87342   0.85185       158\n",
      "\n",
      "    accuracy                        0.98609     10207\n",
      "   macro avg    0.78378   0.88109   0.82534     10207\n",
      "weighted avg    0.98716   0.98609   0.98649     10207\n",
      "\n",
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "40827/40827 [==============================] - 33s 810us/sample - loss: 0.0182 - acc: 0.9939 - val_loss: 0.0689 - val_acc: 0.9853\n",
      "Epoch 2/10\n",
      "40827/40827 [==============================] - 32s 791us/sample - loss: 0.0154 - acc: 0.9947 - val_loss: 0.0833 - val_acc: 0.9857\n",
      "Epoch 3/10\n",
      "40827/40827 [==============================] - 33s 810us/sample - loss: 0.0191 - acc: 0.9936 - val_loss: 0.0716 - val_acc: 0.9868\n",
      "Epoch 4/10\n",
      "40827/40827 [==============================] - 33s 797us/sample - loss: 0.0139 - acc: 0.9947 - val_loss: 0.0775 - val_acc: 0.9857\n",
      "Epoch 5/10\n",
      "40827/40827 [==============================] - 32s 796us/sample - loss: 0.0146 - acc: 0.9951 - val_loss: 0.0883 - val_acc: 0.9847\n",
      "Epoch 6/10\n",
      "40827/40827 [==============================] - 33s 800us/sample - loss: 0.0167 - acc: 0.9944 - val_loss: 0.0811 - val_acc: 0.9831\n",
      "Epoch 7/10\n",
      "40827/40827 [==============================] - 33s 806us/sample - loss: 0.0171 - acc: 0.9939 - val_loss: 0.0801 - val_acc: 0.9833\n",
      "Epoch 8/10\n",
      "40827/40827 [==============================] - 33s 808us/sample - loss: 0.0151 - acc: 0.9949 - val_loss: 0.0729 - val_acc: 0.9863\n",
      "Epoch 9/10\n",
      "40827/40827 [==============================] - 32s 793us/sample - loss: 0.0141 - acc: 0.9950 - val_loss: 0.0790 - val_acc: 0.9864\n",
      "Epoch 10/10\n",
      "40827/40827 [==============================] - 32s 787us/sample - loss: 0.0124 - acc: 0.9956 - val_loss: 0.0794 - val_acc: 0.9849\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99598   0.99006   0.99301      9755\n",
      "           1    0.77778   0.91304   0.84000        23\n",
      "           2    0.81395   0.91623   0.86207       191\n",
      "           3    0.74627   0.79365   0.76923        63\n",
      "           4    0.62857   0.88000   0.73333        25\n",
      "           5    0.76506   0.84667   0.80380       150\n",
      "\n",
      "    accuracy                        0.98491     10207\n",
      "   macro avg    0.78793   0.88994   0.83357     10207\n",
      "weighted avg    0.98625   0.98491   0.98542     10207\n",
      "\n",
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "40827/40827 [==============================] - 33s 799us/sample - loss: 0.0157 - acc: 0.9946 - val_loss: 0.0813 - val_acc: 0.9851\n",
      "Epoch 2/10\n",
      "40827/40827 [==============================] - 33s 812us/sample - loss: 0.0130 - acc: 0.9954 - val_loss: 0.0792 - val_acc: 0.9859\n",
      "Epoch 3/10\n",
      "40827/40827 [==============================] - 32s 794us/sample - loss: 0.0125 - acc: 0.9957 - val_loss: 0.0798 - val_acc: 0.9871\n",
      "Epoch 4/10\n",
      "40827/40827 [==============================] - 32s 781us/sample - loss: 0.0118 - acc: 0.9957 - val_loss: 0.0781 - val_acc: 0.9862\n",
      "Epoch 5/10\n",
      "40827/40827 [==============================] - 33s 798us/sample - loss: 0.0111 - acc: 0.9963 - val_loss: 0.0925 - val_acc: 0.9853\n",
      "Epoch 6/10\n",
      "40827/40827 [==============================] - 33s 808us/sample - loss: 0.0121 - acc: 0.9957 - val_loss: 0.0915 - val_acc: 0.9843\n",
      "Epoch 7/10\n",
      "40827/40827 [==============================] - 34s 834us/sample - loss: 0.0111 - acc: 0.9963 - val_loss: 0.0892 - val_acc: 0.9852\n",
      "Epoch 8/10\n",
      "40827/40827 [==============================] - 34s 824us/sample - loss: 0.0123 - acc: 0.9956 - val_loss: 0.0806 - val_acc: 0.9866\n",
      "Epoch 9/10\n",
      "40827/40827 [==============================] - 33s 817us/sample - loss: 0.0116 - acc: 0.9956 - val_loss: 0.0859 - val_acc: 0.9855\n",
      "Epoch 10/10\n",
      "40827/40827 [==============================] - 33s 805us/sample - loss: 0.0104 - acc: 0.9963 - val_loss: 0.0906 - val_acc: 0.9855\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99670   0.98986   0.99327      9764\n",
      "           1    0.74074   0.90909   0.81633        22\n",
      "           2    0.80930   0.91099   0.85714       191\n",
      "           3    0.67164   0.88235   0.76271        51\n",
      "           4    0.68571   0.85714   0.76190        28\n",
      "           5    0.78916   0.86755   0.82650       151\n",
      "\n",
      "    accuracy                        0.98550     10207\n",
      "   macro avg    0.78221   0.90283   0.83631     10207\n",
      "weighted avg    0.98709   0.98550   0.98609     10207\n",
      "\n",
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "40827/40827 [==============================] - 34s 822us/sample - loss: 0.0103 - acc: 0.9965 - val_loss: 0.0838 - val_acc: 0.9861\n",
      "Epoch 2/10\n",
      "40827/40827 [==============================] - 33s 808us/sample - loss: 0.0095 - acc: 0.9969 - val_loss: 0.0893 - val_acc: 0.9858\n",
      "Epoch 3/10\n",
      "40827/40827 [==============================] - 33s 796us/sample - loss: 0.0106 - acc: 0.9964 - val_loss: 0.0987 - val_acc: 0.9835\n",
      "Epoch 4/10\n",
      "40827/40827 [==============================] - 33s 806us/sample - loss: 0.0094 - acc: 0.9968 - val_loss: 0.0925 - val_acc: 0.9860\n",
      "Epoch 5/10\n",
      "40827/40827 [==============================] - 33s 805us/sample - loss: 0.0093 - acc: 0.9966 - val_loss: 0.0935 - val_acc: 0.9851\n",
      "Epoch 6/10\n",
      "40827/40827 [==============================] - 33s 804us/sample - loss: 0.0091 - acc: 0.9967 - val_loss: 0.0899 - val_acc: 0.9855\n",
      "Epoch 7/10\n",
      "40827/40827 [==============================] - 33s 801us/sample - loss: 0.0113 - acc: 0.9961 - val_loss: 0.1037 - val_acc: 0.9848\n",
      "Epoch 8/10\n",
      "40827/40827 [==============================] - 33s 801us/sample - loss: 0.0095 - acc: 0.9967 - val_loss: 0.0850 - val_acc: 0.9874\n",
      "Epoch 9/10\n",
      "40827/40827 [==============================] - 33s 798us/sample - loss: 0.0083 - acc: 0.9970 - val_loss: 0.0951 - val_acc: 0.9859\n",
      "Epoch 10/10\n",
      "40827/40827 [==============================] - 33s 811us/sample - loss: 0.0116 - acc: 0.9960 - val_loss: 0.0997 - val_acc: 0.9847\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99515   0.99107   0.99310      9737\n",
      "           1    0.62963   1.00000   0.77273        17\n",
      "           2    0.87907   0.83260   0.85520       227\n",
      "           3    0.64179   0.82692   0.72269        52\n",
      "           4    0.57143   0.86957   0.68966        23\n",
      "           5    0.79518   0.87417   0.83281       151\n",
      "\n",
      "    accuracy                        0.98472     10207\n",
      "   macro avg    0.75204   0.89905   0.81103     10207\n",
      "weighted avg    0.98625   0.98472   0.98524     10207\n",
      "\n",
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "40827/40827 [==============================] - 32s 785us/sample - loss: 0.0089 - acc: 0.9970 - val_loss: 0.0838 - val_acc: 0.9855\n",
      "Epoch 2/10\n",
      "40827/40827 [==============================] - 33s 797us/sample - loss: 0.0079 - acc: 0.9973 - val_loss: 0.0981 - val_acc: 0.9839\n",
      "Epoch 3/10\n",
      "40827/40827 [==============================] - 33s 803us/sample - loss: 0.0086 - acc: 0.9970 - val_loss: 0.0953 - val_acc: 0.9846\n",
      "Epoch 4/10\n",
      "40827/40827 [==============================] - 34s 821us/sample - loss: 0.0089 - acc: 0.9968 - val_loss: 0.1004 - val_acc: 0.9845\n",
      "Epoch 5/10\n",
      "40827/40827 [==============================] - 34s 841us/sample - loss: 0.0083 - acc: 0.9969 - val_loss: 0.0882 - val_acc: 0.9863\n",
      "Epoch 6/10\n",
      "40827/40827 [==============================] - 34s 838us/sample - loss: 0.0075 - acc: 0.9974 - val_loss: 0.0939 - val_acc: 0.9860\n",
      "Epoch 7/10\n",
      "40827/40827 [==============================] - 34s 826us/sample - loss: 0.0076 - acc: 0.9974 - val_loss: 0.1064 - val_acc: 0.9848\n",
      "Epoch 8/10\n",
      "40827/40827 [==============================] - 34s 821us/sample - loss: 0.0071 - acc: 0.9978 - val_loss: 0.0968 - val_acc: 0.9865\n",
      "Epoch 9/10\n",
      "40827/40827 [==============================] - 35s 853us/sample - loss: 0.0074 - acc: 0.9972 - val_loss: 0.0959 - val_acc: 0.9859\n",
      "Epoch 10/10\n",
      "40827/40827 [==============================] - 34s 845us/sample - loss: 0.0087 - acc: 0.9970 - val_loss: 0.1080 - val_acc: 0.9840\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99649   0.98794   0.99220      9781\n",
      "           1    0.70370   1.00000   0.82609        19\n",
      "           2    0.77674   0.90270   0.83500       185\n",
      "           3    0.59701   0.93023   0.72727        43\n",
      "           4    0.65714   0.95833   0.77966        24\n",
      "           5    0.79518   0.85161   0.82243       155\n",
      "\n",
      "    accuracy                        0.98403     10207\n",
      "   macro avg    0.75438   0.93847   0.83044     10207\n",
      "weighted avg    0.98643   0.98403   0.98484     10207\n",
      "\n",
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "40827/40827 [==============================] - 35s 848us/sample - loss: 0.0086 - acc: 0.9969 - val_loss: 0.0984 - val_acc: 0.9828\n",
      "Epoch 2/10\n",
      "40827/40827 [==============================] - 34s 843us/sample - loss: 0.0068 - acc: 0.9977 - val_loss: 0.1079 - val_acc: 0.9848\n",
      "Epoch 3/10\n",
      "40827/40827 [==============================] - 35s 857us/sample - loss: 0.0071 - acc: 0.9973 - val_loss: 0.0957 - val_acc: 0.9863\n",
      "Epoch 4/10\n",
      "40827/40827 [==============================] - 34s 837us/sample - loss: 0.0069 - acc: 0.9978 - val_loss: 0.0979 - val_acc: 0.9826\n",
      "Epoch 5/10\n",
      "40827/40827 [==============================] - 34s 842us/sample - loss: 0.0079 - acc: 0.9973 - val_loss: 0.0961 - val_acc: 0.9864\n",
      "Epoch 6/10\n",
      "40827/40827 [==============================] - 34s 839us/sample - loss: 0.0061 - acc: 0.9980 - val_loss: 0.1043 - val_acc: 0.9855\n",
      "Epoch 7/10\n",
      "40827/40827 [==============================] - 35s 853us/sample - loss: 0.0072 - acc: 0.9975 - val_loss: 0.1164 - val_acc: 0.9836\n",
      "Epoch 8/10\n",
      "40827/40827 [==============================] - 34s 835us/sample - loss: 0.0090 - acc: 0.9972 - val_loss: 0.0967 - val_acc: 0.9858\n",
      "Epoch 9/10\n",
      "40827/40827 [==============================] - 35s 862us/sample - loss: 0.0064 - acc: 0.9978 - val_loss: 0.0948 - val_acc: 0.9863\n",
      "Epoch 10/10\n",
      "40827/40827 [==============================] - 34s 843us/sample - loss: 0.0053 - acc: 0.9981 - val_loss: 0.1002 - val_acc: 0.9857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99618   0.99036   0.99327      9754\n",
      "           1    0.62963   0.94444   0.75556        18\n",
      "           2    0.83721   0.89552   0.86538       201\n",
      "           3    0.67164   0.83333   0.74380        54\n",
      "           4    0.57143   1.00000   0.72727        20\n",
      "           5    0.83735   0.86875   0.85276       160\n",
      "\n",
      "    accuracy                        0.98570     10207\n",
      "   macro avg    0.75724   0.92207   0.82301     10207\n",
      "weighted avg    0.98737   0.98570   0.98628     10207\n",
      "\n",
      "Train on 40827 samples, validate on 10207 samples\n",
      "Epoch 1/10\n",
      "40827/40827 [==============================] - 35s 858us/sample - loss: 0.0064 - acc: 0.9978 - val_loss: 0.1039 - val_acc: 0.9859\n",
      "Epoch 2/10\n",
      "40827/40827 [==============================] - 35s 855us/sample - loss: 0.0068 - acc: 0.9979 - val_loss: 0.1066 - val_acc: 0.9848\n",
      "Epoch 3/10\n",
      "40827/40827 [==============================] - 35s 861us/sample - loss: 0.0056 - acc: 0.9979 - val_loss: 0.1032 - val_acc: 0.9860\n",
      "Epoch 4/10\n",
      "40827/40827 [==============================] - 35s 847us/sample - loss: 0.0056 - acc: 0.9980 - val_loss: 0.1094 - val_acc: 0.9857\n",
      "Epoch 5/10\n",
      "23680/40827 [================>.............] - ETA: 13s - loss: 0.0058 - acc: 0.9981"
     ]
    }
   ],
   "source": [
    "val_acc=[]\n",
    "acc=[]\n",
    "val_loss=[]\n",
    "loss=[]\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "model = build_model(28, 28, 3, 6,0.3)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\",metrics=[\"acc\"])\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
    "y_predict=model.predict(x_test).argmax(axis=1)\n",
    "val_acc.append(history.history[\"val_acc\"])\n",
    "acc.append(history.history[\"acc\"])\n",
    "val_loss.append(history.history[\"val_loss\"])\n",
    "loss.append(history.history[\"loss\"])\n",
    "print(metrics.classification_report(y_predict,y_test,digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-085a10ee5682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mval_acc_liste\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0macc_liste\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_acc' is not defined"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val_acc_liste = list(itertools.chain(*val_acc))\n",
    "acc_liste = list(itertools.chain(*acc))\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "plt.plot(acc_liste,label=\"train\");\n",
    "plt.plot(val_acc_liste,label=\"test\");\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Accuracy en fonction du nombre d'epoch\");\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-38142b768be0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_loss_liste\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss_liste\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_loss' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "val_loss_liste = list(itertools.chain(*val_loss))\n",
    "loss_liste = list(itertools.chain(*loss))\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "plt.plot(loss_liste,label=\"train\");\n",
    "plt.plot(val_loss_liste,label=\"test\");\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Accuracy en fonction du nombre d'epoch\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.99219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
